{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b2pJReHgDnEA"
   },
   "source": [
    "# Семинар : Градиентный спуск\n",
    "\n",
    "## Вступление\n",
    "\n",
    "Градиентный спуск - это алгоритм оптимизации, т.е. численный метод поиска локального минимума/максимума функции. \n",
    "\n",
    "### План семинара\n",
    "\n",
    "1. Введение\n",
    "2. Суть метода градиентного спуска\n",
    "3. Модификации градиентного спуска\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4cVcPZTKgYyW"
   },
   "source": [
    "## 1. Введение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J0p0NJTegbsl"
   },
   "source": [
    "\n",
    "##### Наводящие вопросы из геометрии:\n",
    "*  Каким уравнением задается прямая на плоскости? Чем отличаются записи $y = kx + b$ и $ax + by + c = 0$?\n",
    "\n",
    "* Запишите уравнение плоскости в трехмерном пространстве, уравнение гиперплоскости в многомерном пространстве. Используя введенные обозначения, ответьте, в пространстве какой размерности задается гиперплоскость из предыдущего вопроса?\n",
    "\n",
    "* Если какой-то из коэффициентов в уравнении гиперплоскости равен 0, что это геометрически означает?\n",
    "\n",
    "* Что означает, что свободный член равен 0?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hk_L_elFDnEJ"
   },
   "source": [
    "### Визуализация функции от двух переменных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vQiMdcLRDnEJ"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from matplotlib import cm\n",
    "#from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YU5c1tNCESj5"
   },
   "source": [
    "Ниже приведена функция, визуализирующая поверхности. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "daqiLsS-DnEK"
   },
   "outputs": [],
   "source": [
    "def plot_3d(fun, a=-1, b=1, c=-1, d=1, trace=None): #двумерная пов-ть в трехмерном пр-ве\n",
    "    \"\"\"\n",
    "    Визуализирует функцию fun на квадрате [a, b] x [c, d]\n",
    "    fun : функция, принимающая два аргумента\n",
    "         (np.array одинакового размера) и возвращающая\n",
    "          np.array того же размера со значениями функции\n",
    "          в соответствующих точках\n",
    "    Дополнительно можно нарисовать ломаную линию из N точек,\n",
    "    лежащую на получившейся поверхности\n",
    "    trace : np.array размера N x 2 - координаты на плоскости,\n",
    "            обозначающие точки ломаной\n",
    "    \"\"\"\n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "\n",
    "    # Make grid\n",
    "    x1_ = np.linspace(a, b, 100)\n",
    "    x2_ = np.linspace(c, d, 100)\n",
    "    x1, x2 = np.meshgrid(x1_, x2_)\n",
    "    y = fun(x1, x2)\n",
    "\n",
    "    # Plot the surface\n",
    "    ax = fig.add_subplot(1, 1, 1, projection=\"3d\")\n",
    "    ax.plot_surface(x1, x2, y, alpha=0.6)\n",
    "    ax.contour(x1, x2, y, zdir=\"z\", offset=y.min(), cmap=cm.coolwarm)\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    \n",
    "    # Plot 3d line\n",
    "    if trace is not None:\n",
    "        y_trace = fun(trace[:, 0], trace[:, 1])\n",
    "        ax.plot(trace[:, 0], trace[:, 1], y_trace, \"o-\")\n",
    "        ax.set_xlim(x1.min(), x1.max())\n",
    "        ax.set_ylim(x2.min(), x2.max())\n",
    "        ax.set_zlim(y.min(), y.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(not None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRmjPKGHDnEK"
   },
   "source": [
    "\n",
    "\n",
    "Нарисуем с ее помощью параболоид:\n",
    "\n",
    "$$z=x^2+y^2$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "id": "7Wk0bjzIDnEL",
    "outputId": "39fecd3f-5499-4d72-cd76-af1363ac97c9"
   },
   "outputs": [],
   "source": [
    "fun = lambda x1, x2: x1**2 + x2**2\n",
    "plot_3d(fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y1dt9teDDnEM"
   },
   "source": [
    "Круги на плоскости показывают __проекции линий уровня__ параболоида."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZjuGmvtgDnEM"
   },
   "source": [
    "**Задание**: нарисуйте плоскость $y = x_1 + 2 x_2 + 3$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "id": "hhY0nvnrDnEO",
    "outputId": "80e600fa-0e11-498a-fde8-6052dffaadd3"
   },
   "outputs": [],
   "source": [
    "fun = lambda x1, x2: x1 + 2*x2 + 3 #- x1 - x2 + 1\n",
    "# fun = # your code\n",
    "plot_3d(fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mjp9QDOrDnEP"
   },
   "source": [
    "**Задание**: нарисуйте плоскость, параллельную любой из горизонтальных осей:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "id": "cYUuZAmjDnEP",
    "outputId": "5b8c60dc-b1d6-4f20-cf80-c056ad022904"
   },
   "outputs": [],
   "source": [
    "fun = lambda x1, x2: x1+1 #0*x1+0*x2+1 #параллельно y\n",
    "# fun = # your code\n",
    "plot_3d(fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k7eGDvG1DnEP"
   },
   "source": [
    "**Задание**: нарисуйте плоскость, проходящую через начало координат:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "id": "KE97nGN1DnEQ",
    "outputId": "e0a8ebec-7c34-4d58-a106-b5bef5dd4b36"
   },
   "outputs": [],
   "source": [
    "fun = lambda x1, x2: x1 + x2 \n",
    "# fun = # your code\n",
    "plot_3d(fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p36aZltGguEG"
   },
   "source": [
    "## 2. Суть метода градиентного спуска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xYj8qYNxDnEQ"
   },
   "source": [
    "### Градиентный спуск, теоретическая часть\n",
    "\n",
    "Градиент функции $f(x) = f(x_1, \\dots, x_d)$ от многих переменных в точке $x_0$ - это вектор ее частных производных, вычисленных в точке $x_0$.\n",
    "$$\\nabla_x f \\bigl | _{x_0} = \\biggl(\\frac{\\partial f}{\\partial x_1}, \\dots, \\frac{\\partial f}{\\partial x_d} \\biggr ) \\biggl | _{x_0}$$\n",
    "\n",
    "Разберем два простых примера вычисления градиента в случае функции от двух переменных."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AgDX2E-yhN4F"
   },
   "source": [
    "#### Задача 1\n",
    "\n",
    "Найдите градиент линейной функции $f(x) = f(x_1, x_2) = c_1 x_1 + c_2 x_2$ ($c_1$ и $c_2$ - фиксированные числа). \n",
    "\n",
    "__Решение.__\n",
    "\n",
    "Найдем первую частную производную: \n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x_1} = \\frac{\\partial (c_1 x_1 + c_2 x_2)}{\\partial x_1} = c_1.$$\n",
    "\n",
    "Значит, первая компонента градиента равна $c_1$. Аналогично со второй компонентой. \n",
    "\n",
    "Ответ:\n",
    "\n",
    "$$\\nabla_x f = (c_1, c_2)$$\n",
    "\n",
    "Можно подставить конкретные коэффициенты, например $c_1 = 3$ и $c_2 = 7$. Тогда градиент будет равен $(3, 7)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pna1-g9AhTyu"
   },
   "source": [
    "#### Задача 2\n",
    "Найдите градиент квадратичной функции $f(x) = f(x_1, x_2) = c_1 x_1^2 + c_2 x_2^2$ ($c_1$ и $c_2$ - фиксированные числа). \n",
    "\n",
    "__Решение.__\n",
    "Найдем первую частную производную: \n",
    "\n",
    "$$\\frac{\\partial f}{\\partial x_1} = \\frac{\\partial (c_1 x_1^2 + c_2 x_2^2)}{\\partial x_1} = 2 c_1 x_1.$$\n",
    "\n",
    "Значит, первая компонента градиента равна $2 c_1 x_1$. Аналогично со второй компонентой. \n",
    "\n",
    "Ответ:\n",
    "\n",
    "$$\\nabla_x f = (2 c_1 x_1, 2 c_2 x_2)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lNxxf4qYDnER"
   },
   "source": [
    "При неотрицательных $c_1, c_2$ минимум такой квадратичной функции достигается в 0.\n",
    "Наша следующая цель - найти этот минимум с помощью градиентного спуска. \n",
    "\n",
    "##### Вопросы:\n",
    "* Какую (оптимизационную) задачу решает градиентный спуск?\n",
    "* Как работает алгоритм градиентного спуска?\n",
    "* Как выбирать начальную инициализацию в градиентном спуске?\n",
    "* Когда останавливать градиентный спуск?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Um3nYy14DnER"
   },
   "source": [
    "### Градиент квадратичной функции\n",
    "\n",
    "Воспользуемся кодом для вычисления квадратичной функции  и обобщим ее на случай произвольных коэффициентов:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YEkD4cR7DnER"
   },
   "outputs": [],
   "source": [
    "def fun(x1, x2, c1=1, c2=1):\n",
    "    return c1 * (x1**2) + c2 * (x2**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pr__YZ65DnER"
   },
   "source": [
    "Эта функция способна обрабатывать x1 и x2 любой размерности, но градиенты будем считать в предположении, что x1 и x2 - числа "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E5rwwKfmDnES"
   },
   "source": [
    "Реализуем подсчет градиента функции fun. Код функции вычисления градиента в одной точке (согласно описанию):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sicdApN_DnES"
   },
   "outputs": [],
   "source": [
    "def grad_fun(x1, x2, c1=1, c2=1):\n",
    "    \"\"\"\n",
    "    Функция берет 2 числа, обозначающую точку, в которой вычисляется градиент,\n",
    "    и возвращает np.array размера (2,) - градиент квадратичной функции\n",
    "    Опциональные аргументы: c1 и c2 - коэффициенты\n",
    "    \"\"\"\n",
    "    return np.array([2*c1*x1, 2*c2*x2])\n",
    "        \n",
    "    #    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dc8UNdL6DnES"
   },
   "source": [
    "Проверим:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BN5-hkFODnES",
    "outputId": "8a71b984-be1b-4871-ac32-af7d1149d479"
   },
   "outputs": [],
   "source": [
    "#ответ array([2., 9.])\n",
    "grad_fun(x1=0.5, x2=1.5, c1=2, c2=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O6v2Bh8LDnET"
   },
   "source": [
    "### Градиентный спуск (GD), реализация\n",
    "\n",
    "__Реализуем градиентный спуск__. \n",
    "Он работает следующим образом: \n",
    "1. сначала инициализируется начальная точка x \n",
    "1. затем повторяются итерации (общая формула для минимизации $f$):\n",
    "$$x^{(t)} = x^{(t-1)} - \\alpha \\nabla_{x^{(t-1)}} f$$\n",
    "Здесь $\\alpha$ - длина шага.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3b4cPEcWDnET"
   },
   "outputs": [],
   "source": [
    "def grad_descend(grad_fun, step_size=0.1, num_steps=50):\n",
    "    \"\"\"\n",
    "    Реализует градиентный спуск\n",
    "    Аргументы:\n",
    "    * grad_fun - функция, вычисляющая градиент\n",
    "    * step_size - длина шага\n",
    "    * num_steps - число итераций\n",
    "\n",
    "    Возвращает np.array размера (num_steps+1) x 2,\n",
    "    (i+1)-й элемент - точка на (i+1)-й итерации,\n",
    "    нулевой элемент - случайная инициализация\n",
    "    \"\"\"\n",
    "    #массив чисел из равн. распр. в диапазоне [0, 1) размера (2,)\n",
    "    x = np.random.rand(2) * 4 - 2\n",
    "    \n",
    "    trace = np.zeros((num_steps + 1, 2))\n",
    "    trace[0] = x        #начальное приближение\n",
    "    for it in range(num_steps):\n",
    "        grad = grad_fun(x[0], x[1]) #градиент\n",
    "        \n",
    "        x = x - step_size * grad #градиентный шаг\n",
    "        \n",
    "        trace[it + 1] = x #приближение к min на шаге it + 1\n",
    "    return trace        \n",
    "    \n",
    "#    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "unnJL0I_DnET"
   },
   "source": [
    "Протестируем функцию (последний элемент должен быть близок к 0):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sJmL1z9ADnET"
   },
   "outputs": [],
   "source": [
    "np.random.seed(16)\n",
    "\n",
    "trace = grad_descend(grad_fun)\n",
    "trace"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xeFeOdEADnEU"
   },
   "source": [
    "__Визуализируем градиентный спуск__. Для этого передадим нашу траекторию оптимизации в качестве последнего аргумента функции plot_3d."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 248
    },
    "id": "dDMZ5Sd4DnEU",
    "outputId": "910e1964-0037-413e-d7ba-674073cd3e69"
   },
   "outputs": [],
   "source": [
    "trace = grad_descend(grad_fun, 0.1, 30)\n",
    "print(trace[0])\n",
    "print(trace[30])\n",
    "plot_3d(fun, trace=trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B_preOQ7DnEU"
   },
   "source": [
    "Запустим оптимизацию несколько раз, чтобы посмотреть, __как ведет себя процесс при различных случайных начальных приближениях__ (вообще говоря, сходимость градиентного спуска зависит от выбора начального приближения):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 710
    },
    "id": "odgD813xDnEV",
    "outputId": "5d5fa084-4185-4407-c427-e7ab3b05dc49"
   },
   "outputs": [],
   "source": [
    "for i in range(3):\n",
    "    trace = grad_descend(grad_fun, 0.1, 30)\n",
    "    plot_3d(fun, trace=trace)\n",
    "    print(trace[0])\n",
    "    print(trace[30])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BFVbk20kDnEW"
   },
   "source": [
    "__Используем разную длину шага__ из множества $(0.01, 0.1, 0.5, 1)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 941
    },
    "id": "oakj53AODnEW",
    "outputId": "9162d8c7-5a6d-4c50-bc1f-cc32e2041e82"
   },
   "outputs": [],
   "source": [
    "for ss in [0.01, 0.1, 0.5, 1]:\n",
    "    np.random.seed(0)\n",
    "    trace = grad_descend(grad_fun, step_size=ss)\n",
    "    plot_3d(fun, trace=trace)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwQpsbCZDnEX"
   },
   "source": [
    "При маленькой длине шага процесс идет слишком медленно, при большой - может разойтись."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wkIFXKuJDnEX"
   },
   "source": [
    "Наконец, __попробуем использовать другие коэффициенты квадратичной функции__. \n",
    "Оптимизируем функцию $f(x) = x_1^2 + 5 x_2^2$, пробуя длину шага (0.1, 0.2, 0.5):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 710
    },
    "id": "qSl9_u44DnEX",
    "outputId": "c70f4c95-828c-46c3-f24d-d65e876e898d"
   },
   "outputs": [],
   "source": [
    "fun_c = lambda x1, x2: fun(x1, x2, 1, 5)\n",
    "grad_fun_c = lambda x1, x2: grad_fun(x1, x2, 1, 5)\n",
    "\n",
    "for ss in [0.1, 0.2, 0.5]:\n",
    "    trace = grad_descend(grad_fun_c, step_size=ss)\n",
    "    plot_3d(fun_c, trace=trace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_eLSNNy7DnEY"
   },
   "source": [
    "\"Вытянутую\" функцию сложнее оптимизировать. Именно поэтому __данные рекомендуется масштабировать__ (нормировать) перед обучением модели, чтобы избежать таких колебаний при оптимизации."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o_SM9y0tDnEY"
   },
   "source": [
    "###  Градиентый спуск для оценки параметров линейной регрессии \n",
    "На простом примере разберём основные тонкости."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RX_F7mAKDnEY"
   },
   "source": [
    "__Сгенерируем матрицу объекты-признаки $X$ и вектор весов $w_{true}$__, __вектор целевых переменных $y$__ будем вычислять как:\n",
    "\n",
    "$$\n",
    "y = Xw_{true} + \\epsilon,\n",
    "$$\n",
    "где $\\epsilon \\sim N(0, 1)$ (нормальный шум)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I039ksbrgOTq"
   },
   "outputs": [],
   "source": [
    "np.random.seed(25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S8dgJUUPDnEZ"
   },
   "outputs": [],
   "source": [
    "#генерируем данные (сгенерируем линейную зависимость с шумом)\n",
    "n_features = 2\n",
    "n_objects = 300 \n",
    "batch_size = 10\n",
    "num_steps = 43\n",
    "\n",
    "w_true = np.random.normal(size=(n_features,)) #вектор весов w_true  #normal(loc=0.0, scale=1.0, size) \n",
    "\n",
    "X = np.random.uniform(-5, 5, (n_objects, n_features)) #uniform(a, b, shape)\n",
    "X *= (np.arange(n_features) * 2 + 1)[np.newaxis, :] #поэлементное умножение, X[n_objects, n_features] - матр. объект-признак\n",
    "Y = X.dot(w_true) + np.random.normal(0, 1, (n_objects)) #ответы y=Xw_true+eps #normal(loc, scale, size)\n",
    "w_0 = np.random.uniform(-2, 2, (n_features)) #начальное приближение для весов - массив np.array (n_features,) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GuhJjhlwNcv"
   },
   "source": [
    "Напомним, что\n",
    "$Q(a) = \\frac 1 \\ell \\sum_{i=1}^\\ell L(y_i, a(x_i))$ - функционал ошибки, или в линейной регрессии Mean Squared Error (MSE):\n",
    "$$Q (a, X) = \\frac{1}{l}\\sum^l_{i=1}(a(x_i) - y_i)^2,$$\n",
    "где $a(x)=  w_0+w_1x_1+...w_dx_d$ - приближение (алгоритм), или\n",
    "$$Q(w)=\n",
    "\\frac{1}{l}\\sum_{i=1}^{l}(\\langle w,x_i \\rangle -y_i)^2\\rightarrow min_{w_0,w_1,...,w_d}\n",
    "$$\n",
    "\n",
    "где $x_i$ — это $i$-ый объект датасета, $y_i$ — правильный ответ для $i$-го объекта, а $w$ — веса нашей линейной модели.\n",
    "\n",
    "Мы решаем __задачу минимизации функционала__ $Q$, т.е. \n",
    "$$Q(w)=\n",
    "\\frac{1}{l}\\sum_{i=1}^{l}(\\langle w,x_i \\rangle -y_i)^2\\rightarrow min_{w_0,w_1,...,w_d}\n",
    "$$\n",
    "Если мы попробуем минимизировать $Q$ \"в лоб\", то __оптимальный набор параметров__ будет выглядеть так: \n",
    "$$w = (X^TX)^{-1}X^Ty$$ В этой формуле присутствует обращение матрицы $X^TX$ — очень трудоёмкая операция при большом количестве признаков. Сложность вычислений в этом случае равна $O(d^3 + d^2 \\ell)$, где $l$ - число объектов, $d$ — число признаков. При решении задач такая трудоёмкость часто оказывается непозволительной, поэтому параметры ищут итерационными методами, стоимость которых меньше.   \n",
    "Один из них — __градиентный спуск__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Knp-DJSDDnEZ"
   },
   "source": [
    "В __градиентном спуске__ значения параметров на следующем шаге получаются из значений параметров на текущем шаге смещением в сторону антиградиента функционала $Q(w)$: \n",
    "\n",
    "$$w^{(t+1)} = w^{(t)} - \\eta_t \\nabla Q(w^{(t)}),$$\n",
    "где $\\eta_t$ — длина шага градиентного спуска, $\\nabla_w Q(w) =\\frac{1}{l}\\sum_{i=1}^{l}\\nabla_w L_i(w)$.\n",
    "\n",
    "Функционал MSE в матричном виде можно записать так:\n",
    "$$\n",
    "Q(w) =\\frac{1}{l} (y - Xw)^T(y-Xw)\n",
    "$$\n",
    "или\n",
    "$$\n",
    "Q(w, X, y) = \\frac{1}{l} || Xw - y ||^2,\n",
    "$$\n",
    "где $X$ — это матрица объекты-признаки, а $y$ — вектор правильных ответов.\n",
    "А соответствующий градиент:\n",
    "$$\n",
    "\\nabla_w Q(w) =\\frac{1}{l} \\nabla_w[y^Ty - y^TXw - w^TX^Ty + w^TX^TXw] =\\frac{1}{l}( 0 - X^Ty - X^Ty + (X^TX + X^TX)w) = \\frac{2}{l}X^T(Xw-y), т.е.\n",
    "$$\n",
    " \n",
    "$$\n",
    "\\nabla_w Q(w) =\\frac{2}{l}X^T(Xw-y)\n",
    "$$\n",
    "\n",
    "Сложность вычислений при нахождении параметров в данном случае равна $O(d \\ell)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B--w2SLbzqPd"
   },
   "source": [
    "__Обучим на полученных данных линейную регрессию для MSE при помощи полного градиентного спуска__.  \n",
    "В результате, получим вектор оценок параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XtM1n2BmDnEa"
   },
   "outputs": [],
   "source": [
    "w = w_0.copy() #копия нач. приближения весов\n",
    "w_list = [w.copy()] #список массивов np.array для весов\n",
    "step_size = 1e-2\n",
    "\n",
    "for i in range(num_steps):\n",
    "    #градиентный шаг:\n",
    "    w -= 2 * step_size * np.dot(X.T, np.dot(X, w) - Y) / Y.shape[0] #/ Y.shape[0] - если в Q есть деление на l\n",
    "    w_list.append(w.copy()) #заполняем список массивов np.array для весов\n",
    "w_list = np.array(w_list) #преобразуем список в массив np.array для весов (num_steps,n_features)\n",
    "w_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Применение линейной регрессии \n",
    "from sklearn import linear_model\n",
    "\n",
    "#from sklearn.lenear_model import LinearRegressin #МНК (w по точной формуле) \n",
    "#from sklearn.lenear_model import SGDRegressor #градиентный спуск"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = linear_model.LinearRegression()\n",
    "regr.fit(X, Y)\n",
    "print(\"Coefficients: \\n\", regr.coef_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VA_tsm1hDnEa"
   },
   "source": [
    "### Визуализация траекторий GD\n",
    "\n",
    "__Покажем последовательность оценок параметров $w^{(t)}$, получаемых в ходе итераций__. Красная точка — $w_{true}$.\n",
    "\n",
    "Для этого создадим функцию, которую будем и дальше использовать для визуализации процесса оптимизации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yqLu4ljf5jdj"
   },
   "outputs": [],
   "source": [
    "def plot_gradient(w_list, title):\n",
    "    A, B = np.meshgrid(np.linspace(-2, 2, 100), np.linspace(-2, 2, 100)) #создаем сетку\n",
    "\n",
    "    levels = np.empty_like(A) #линии уровня (массив той же формы и типа, что A)\n",
    "    #для каждой точки сетки считаем Q\n",
    "    for i in range(A.shape[0]):\n",
    "        for j in range(A.shape[1]):\n",
    "            w_tmp = np.array([A[i, j], B[i, j]])\n",
    "            levels[i, j] = np.mean(np.power(np.dot(X, w_tmp) - Y, 2)) #значение (линия уровня Q)\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.title(title)\n",
    "    plt.xlabel(r\"$w_1$\")\n",
    "    plt.ylabel(r\"$w_2$\")\n",
    "    plt.xlim((w_list[:, 0].min() - 0.2, w_list[:, 0].max() + 0.2)) #диапозон по x\n",
    "    plt.ylim((w_list[:, 1].min() - 0.2, w_list[:, 1].max() + 0.2)) #диапозон по y\n",
    "    plt.gca().set_aspect(\"equal\") #оси\n",
    "\n",
    "    # visualize the level set\n",
    "    CS = plt.contour(\n",
    "        A, B, levels, levels=np.logspace(0, 1, num=20), cmap=plt.cm.rainbow_r\n",
    "    ) #линии уровня #np.logspace(0, 1, num=20) от 10^0 до 10^1 - 20 чисел \n",
    "    CB = plt.colorbar(CS, shrink=0.8, extend=\"both\") #шкала\n",
    "\n",
    "    # visualize trajectory\n",
    "    plt.scatter(w_true[0], w_true[1], c=\"r\") #истинные веса (красная точка)\n",
    "    plt.scatter(w_list[:, 0], w_list[:, 1])  #веса, полученные в ходе градиентного спуска - синие точки, их число num_steps\n",
    "    plt.plot(w_list[:, 0], w_list[:, 1])   #соединение точек синими отрезками\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$w^{(t+1)} = w^{(t)} - \\eta_t \\nabla Q(w^{(t)}),$$\n",
    "где $\\eta_t=const$ — длина шага градиентного спуска, $\\nabla_w Q(w) =\\frac{1}{l}\\sum_{i=1}^{l}\\nabla_w L_i(w)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "L3MnbqxC5y4t",
    "outputId": "0d14aa19-074f-4722-86fc-1c83e0973b42"
   },
   "outputs": [],
   "source": [
    "plot_gradient(w_list, \"GD trajectory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HR4zQF5wDnEa"
   },
   "source": [
    "Из мат. анализа известно, градиент перпендикулярен линиям уровня. Это объясняет такие зигзагообразные траектории градиентного спуска. Для большей наглядности в каждой точке пространства __посчитаем градиент функционала и покажем его направление__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "id": "xyqJj5XBDnEb",
    "outputId": "db5e9c1d-86ec-4e62-dd84-b50dbbbecb8c"
   },
   "outputs": [],
   "source": [
    "# compute level set\n",
    "A, B = np.meshgrid(np.linspace(-3, 3, 100), np.linspace(-3, 3, 100)) #создаем сетку\n",
    "A_mini, B_mini = np.meshgrid(np.linspace(-3, 3, 20), np.linspace(-2, 2, 27))\n",
    "\n",
    "levels = np.empty_like(A)\n",
    "#для каждой точки сетки считаем Q\n",
    "for i in range(A.shape[0]):\n",
    "    for j in range(A.shape[1]):\n",
    "        w_tmp = np.array([A[i, j], B[i, j]])\n",
    "        levels[i, j] = np.mean(np.power(np.dot(X, w_tmp) - Y, 2)) #значение (линия уровня Q)\n",
    "\n",
    "# visualize the level set\n",
    "plt.figure(figsize=(13, 9))\n",
    "CS = plt.contour(\n",
    "    A, B, levels, levels=np.logspace(-1, 1.5, num=30), cmap=plt.cm.rainbow_r\n",
    ")                                                #линии уровня\n",
    "CB = plt.colorbar(CS, shrink=0.8, extend=\"both\") #шкала\n",
    "\n",
    "# visualize the gradients\n",
    "gradients = np.empty_like(A_mini)\n",
    "for i in range(A_mini.shape[0]):\n",
    "    for j in range(A_mini.shape[1]):\n",
    "        w_tmp = np.array([A_mini[i, j], B_mini[i, j]])\n",
    "        antigrad = - 2 * 1e-3 * np.dot(X.T, np.dot(X, w_tmp) - Y) / Y.shape[0] #антиград. (с \"-\"), 1e-3 - связь с дл.вектора\n",
    "        plt.arrow(A_mini[i, j], B_mini[i, j], antigrad[0], antigrad[1], head_width=0.02) #стрелки\n",
    "\n",
    "plt.title(\"Antigradients demonstration\")\n",
    "plt.xlabel(r\"$w_1$\")\n",
    "plt.ylabel(r\"$w_2$\")\n",
    "plt.xlim((w_true[0] - 1.5, w_true[0] + 1.5))\n",
    "plt.ylim((w_true[1] - 0.5, w_true[1] + 0.7))\n",
    "plt.gca().set_aspect(\"equal\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0GUBelVYij-Q"
   },
   "source": [
    "## 3. Модификации градиентного спуска"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Vt_qhw1eiXMF"
   },
   "source": [
    "### Стохастический градиентый спуск (SGD)\n",
    "Существуют различные модификации градиентного спуска. \n",
    "Мы познакомимся с одной из них — стохастическим градиентным спуском.\n",
    "От варианта выше он отличается заменой градиента на несмещённую оценку (это точечная оценка, математическое ожидание которой равно оцениваемому параметру) градиента по одному или нескольким объектам. В этом случае сложность становится $O(dk)$, где $k$ — количество объектов, по которым оценивается градиент, $k \\ll \\ell$. Для больших массивов данных стохастический градиентный спуск может дать значительное преимущество в скорости по сравнению со стандартным градиентным спуском.\n",
    "\n",
    "#### Применяем\n",
    "\n",
    "Модифицируем код и будем считать градиент не по всему набору данных, а только __по случайной подвыборке объектов batch__ (в частности, по одному случайному объекту). Получим траектории стохастического градиентного спуска.\n",
    "\n",
    "\n",
    "$$w^{(t+1)} = w^{(t)} - \\eta_t \\frac{1}{k}\\sum_{j=1}^{k}\\nabla_w L_j(w^{(t)}),$$\n",
    "где $\\eta_t=const$ — длина шага градиентного спуска, $j$ - случайно выбранные номера слагаемых из функционала $Q$.\n",
    "\n",
    "\n",
    "__Функция генерации выборки и подсчета градиента__:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bbC5q0PC30J8"
   },
   "outputs": [],
   "source": [
    "def calc_grad_on_batch(X, Y, w, batch_size):\n",
    "    #создаем набор индексов для batch_size\n",
    "    sample = np.random.randint(n_objects, size=batch_size) ##равн. расп. (цел) от 0 до n_objects) размера size\n",
    "    return 2 * np.dot(X[sample].T, np.dot(X[sample], w) - Y[sample]) / batch_size\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_features)\n",
    "print(n_objects) \n",
    "print(batch_size)\n",
    "print(num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Градиентный шаг:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NnTYakQHDnEb"
   },
   "outputs": [],
   "source": [
    "step_size = 1e-2\n",
    "\n",
    "w = w_0.copy() #начальное приближение весов\n",
    "w_list = [w.copy()]\n",
    "\n",
    "for i in range(num_steps):\n",
    "    w -= step_size * calc_grad_on_batch(X, Y, w, batch_size) #градиентный шаг на batch_size, а не на всей выборке\n",
    "    w_list.append(w.copy()) \n",
    "\n",
    "w_list = np.array(w_list) #массив наборов весов (num_steps*2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nLPzSaaBi9k5"
   },
   "source": [
    " ### Визуализация траекторий SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "zYrb2PWt59G6",
    "outputId": "6f7bdf63-2fc1-4bf5-a78a-fc397eca3e7c"
   },
   "outputs": [],
   "source": [
    "plot_gradient(w_list, \"SGD trajectory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7u-T-oIfDnEc"
   },
   "source": [
    "Как видно, метод стохастического градиента «бродит» вокруг оптимума. На такое поведение можно повлиять подбором шага градиентного спуска $\\eta_t$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WgmrMPBdjXDb"
   },
   "source": [
    "### Экспериментируем с градиентным спуском.\n",
    "\n",
    "### ГРАДИЕНТНЫЙ ШАГ\n",
    "\n",
    "__Будем уменьшать шаг градиентного спуска на каждой итерации__.\n",
    "\n",
    "$$\\eta_t=\\frac{s_{0}}{t+1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tWuVCe9vDnEc"
   },
   "outputs": [],
   "source": [
    "step_size_0 = 0.01\n",
    "\n",
    "w = w_0.copy()\n",
    "w_list = [w.copy()]\n",
    "\n",
    "\n",
    "for i in range(num_steps):\n",
    "    step_size = step_size_0 / (i + 1) #уменьшаем шаг\n",
    "    w -= step_size * calc_grad_on_batch(X, Y, w, batch_size) #градиентный шаг на batch_size\n",
    "    w_list.append(w.copy())\n",
    "\n",
    "w_list = np.array(w_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "km2JZ_7N6CIT",
    "outputId": "e1a7cdcb-6f13-4385-c62d-c0b24ccdf7c1"
   },
   "outputs": [],
   "source": [
    "plot_gradient(w_list, \"SGD trajectory with dynamic lr\") #lr - learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Enq5v7Uu4bNW"
   },
   "source": [
    "Теперь градиентный спуск движется более направленно, но не доходит до оптимума.   \n",
    "__Более сложная схему изменения длины шага__:\n",
    "$$\n",
    "    \\eta_t\n",
    "    =\n",
    "    \\lambda\n",
    "    \\left(\n",
    "        \\frac{s_0}{s_0 + t}\n",
    "    \\right)^p.\n",
    "$$\n",
    "Возьмем $s_0 = 1$ и поэкспериментируем с разными $\\lambda$ и $p$.\n",
    "\n",
    "__Функция градиентного шага с изменяемой длиной шага:__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pk_6L-cQ4apO"
   },
   "outputs": [],
   "source": [
    "def sgd_with_lr_schedule(lambda_param, p=0.5, s_init=1.0, batch_size=10):\n",
    "    w = w_0.copy()\n",
    "    w_list = [w.copy()]\n",
    "\n",
    "    for i in range(num_steps):\n",
    "        step_size = lambda_param * np.power(s_init / (s_init + i), p)\n",
    "        w -= step_size * calc_grad_on_batch(X, Y, w, batch_size)\n",
    "        w_list.append(w.copy())\n",
    "\n",
    "    return np.array(w_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "2dnVI8dQ4rcF",
    "outputId": "d1226843-f0e4-4b40-dc2c-6200540abecc"
   },
   "outputs": [],
   "source": [
    "w_list = sgd_with_lr_schedule(lambda_param=0.01, p=0.8)\n",
    "plot_gradient(w_list, f\"SGD with lr shedule, lambda={0.01}, p={0.8}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "ATJBJbRe6QOY",
    "outputId": "20433c38-37af-424c-d2c5-3c3715e4f83b"
   },
   "outputs": [],
   "source": [
    "w_list = sgd_with_lr_schedule(lambda_param=0.01, p=0.5)\n",
    "plot_gradient(w_list, f\"SGD with lr shedule, lambda={0.01}, p={0.5}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "p5h0jEcKAt6U",
    "outputId": "2da6c516-5965-4d2b-db72-8dac19edae57"
   },
   "outputs": [],
   "source": [
    "w_list = sgd_with_lr_schedule(lambda_param=0.01, p=0.1)\n",
    "plot_gradient(w_list, f\"SGD with lr shedule, lambda={0.01}, p={0.1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DCPXDPH4BDy9"
   },
   "source": [
    "Коэффициенты в формуле для длины шага являются гиперпараметрами, и их нужно подбирать. Желательно использовать для этого валидационную выборку."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Cwm0jWOBKX_"
   },
   "source": [
    "### РАЗМЕР ПОДВЫБОРКИ\n",
    "\n",
    "__Посмотрим, как размер подвыборки, по которой оценивается градиент, влияет на сходимость__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "Y4qQqvpu5WH9",
    "outputId": "b01b2a1b-8215-496f-eda6-b61aab66cf11"
   },
   "outputs": [],
   "source": [
    "w_list = sgd_with_lr_schedule(lambda_param=0.01, p=0.35, batch_size=10)\n",
    "plot_gradient(w_list, f\"SGD with batch_size = {10}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 406
    },
    "id": "W9yFUdjd62fu",
    "outputId": "60431d4a-fdd6-40e6-aa4a-ec1229f4ad9a"
   },
   "outputs": [],
   "source": [
    "w_list = sgd_with_lr_schedule(lambda_param=0.01, p=0.35, batch_size=100)\n",
    "plot_gradient(w_list, f\"SGD with batch_size = {100}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_2-iRnA680H"
   },
   "source": [
    "Вывод: чем больше размер подвыборки, тем более стабильная траектория градиентного спуска."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L7nv9KwVDnEd"
   },
   "source": [
    "### Сравнение скоростей сходимости"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S2RkwlE0DnEd"
   },
   "source": [
    "Теперь посмотрим, насколько быстро достигают оптимума методы полного и стохастического градиентного спуска. Сгенерируем выборку и построим два графика: \n",
    " - зависимость значения функционала ошибки (лосса) от номера итерации (шага) градиентного спуска;\n",
    " - зависимость нормы градиента функционала ошибки от номера итерации (шага) градиентного спуска.\n",
    "\n",
    "\n",
    " Норма градиента функционала ошибки вычисляется по формуле:\n",
    "\n",
    " $$\n",
    "||\\nabla Q(w)|| = ||\\frac{2}{l}X^T(Xw-Y)||\n",
    " $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KHlAL9IjDnEe"
   },
   "outputs": [],
   "source": [
    "#генерируем новые данные\n",
    "n_features = 50\n",
    "n_objects = 1000\n",
    "num_steps = 200\n",
    "batch_size = 10\n",
    "\n",
    "#истинные веса\n",
    "w_true = np.random.uniform(-2, 2, n_features) #равном.распр.(low=0.0, high=1.0, size=None)\n",
    "\n",
    "X = np.random.uniform(-10, 10, (n_objects, n_features)) #матр. объект-признак\n",
    "Y = X.dot(w_true) + np.random.normal(0, 5, n_objects) #ответы - normal(loc, scale, size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3bCxC1BBDnEe"
   },
   "outputs": [],
   "source": [
    "from scipy.linalg import norm  #для вычисления нормы матрицы\n",
    "\n",
    "step_size_sgd = 1e-2\n",
    "step_size_gd = 1e-2\n",
    "w_sgd = np.random.uniform(-4, 4, n_features)\n",
    "w_gd = w_sgd.copy()\n",
    "residuals_sgd = [np.mean(np.power(np.dot(X, w_sgd) - Y, 2))]  #список для ошибок (sgd)\n",
    "residuals_gd = [np.mean(np.power(np.dot(X, w_gd) - Y, 2))]   #список для ошибок (gd)\n",
    "\n",
    "norm_sgd = [] #список для норм градиента (sgd)\n",
    "norm_gd = []  #список для норм градиента (gd)\n",
    "\n",
    "\n",
    "for i in range(num_steps):\n",
    "    step_size = step_size_sgd / ((i + 1) ** 0.51) #градиентный шаг (уменьшаем)\n",
    "    sample = np.random.randint(n_objects, size=batch_size) #индексы случайной подвыборки batch_size элементов \n",
    "\n",
    "    w_sgd -= (\n",
    "        2\n",
    "        * step_size\n",
    "        * np.dot(X[sample].T, np.dot(X[sample], w_sgd) - Y[sample])\n",
    "        / batch_size\n",
    "    )      #веса sgd\n",
    "    residuals_sgd.append(np.mean(np.power(np.dot(X, w_sgd) - Y, 2))) #добавляем в список ошибку sgd \n",
    "    norm_sgd.append(norm(np.dot(X[sample].T, np.dot(X[sample], w_sgd) - Y[sample]))) #добавляем норму градиента (sgd) (без const)\n",
    "\n",
    "    w_gd -= 2 * step_size_gd * np.dot(X.T, np.dot(X, w_gd) - Y) / Y.shape[0] #веса gd\n",
    "    residuals_gd.append(np.mean(np.power(np.dot(X, w_gd) - Y, 2))) #добавляем в список ошибку gd\n",
    "    norm_gd.append(norm(np.dot(X.T, np.dot(X, w_gd) - Y)))  #добавляем норму градиента (gd)   (без const)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "0XRxL6OjDnEe",
    "outputId": "e5177bd3-434b-44f5-ca0d-a4289a6083e8"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13, 6))\n",
    "plt.plot(range(num_steps + 1), residuals_gd, label=\"Full GD\")\n",
    "plt.plot(range(num_steps + 1), residuals_sgd, label=\"SGD\")\n",
    "\n",
    "plt.title(\"Значение функционала ошибки (функции потерь) в зависимости от номера итерации (шага)\")\n",
    "plt.xlim((-1, num_steps + 1))\n",
    "plt.legend()\n",
    "plt.xlabel(\"Номер итерации\")\n",
    "plt.ylabel(r\"Q($w$)\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 404
    },
    "id": "-2dBI9DLDnEf",
    "outputId": "d29f5d15-38f6-40c5-97e8-24f587e83235"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(13, 6))\n",
    "plt.plot(range(num_steps), norm_gd, label=\"Full GD\")\n",
    "plt.plot(range(num_steps), norm_sgd, label=\"SGD\")\n",
    "\n",
    "plt.title(\"Норма градиента функционала ошибки (функции потерь) в зависимости от номера итерации (шага))\")\n",
    "plt.xlim((-1, num_steps + 1))\n",
    "plt.legend()\n",
    "plt.xlabel(\"Номер итерации\")\n",
    "plt.ylabel(r\"$||\\nabla Q$($w$)||\")\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dd5yAAlVDnEg"
   },
   "source": [
    "Как видно, GD буквально за несколько итераций оказывается вблизи оптимума, в то время как поведение SGD может быть весьма нестабильным. Нестабильное поведение стохастической модификации на начальных итерациях хорошо заметно на графике с нормой градиента. Как правило, для более сложных моделей наблюдаются ещё большие колебания. Путём подбора величины шага можно добиться лучшей скорости сходимости, и существуют методы, адаптивно подбирающие величину шага (например, AdaGrad, Adam, RMSProp)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1uPpXcuKjzJJ"
   },
   "source": [
    "### Итог\n",
    "\n",
    "Градиентный спуск -- очень мощный инструмент для решения задач машинного обучения. Возможно, это наиболее широко применимый в этой области алгоритм. \n",
    "\n",
    "__Дополнительно.__ Обновляемая статья, в которой собраны наиболее актуальные модификации алгоритма градиентного спуска: https://www.ruder.io/optimizing-gradient-descent/\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "sem04_grad.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
