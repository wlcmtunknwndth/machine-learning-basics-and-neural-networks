{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f46lxLSsBp0f"
   },
   "source": [
    "# Семинар: Градиентный спуск. Задачи "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z8Nfs9riBp0k"
   },
   "outputs": [],
   "source": [
    "#from typing import Iterable, List\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u5OOYVQOBp0l"
   },
   "source": [
    "## Часть 1. Градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7j2vGNTyBp0l"
   },
   "source": [
    "Функционал ошибки, который мы применяем в задаче регрессии — Mean Squared Error:\n",
    "\n",
    "$$\n",
    "Q(w, X, y) = \\frac{1}{\\ell} \\sum\\limits_{i=1}^\\ell (\\langle x_i, w \\rangle - y_i)^2\n",
    "$$\n",
    "\n",
    "где $x_i$ — это $i$-ый объект датасета, $y_i$ — правильный ответ для $i$-го объекта, а $w$ — веса нашей линейной модели.\n",
    "\n",
    "Можно показать, что для линейной модели, функционал ошибки можно записать в матричном виде следующим образом:\n",
    "$$\n",
    "Q(w, X, y) =\\frac{1}{l} (y - Xw)^T(y-Xw)\n",
    "$$\n",
    "или\n",
    "$$\n",
    "Q(w, X, y) = \\frac{1}{l} || Xw - y ||^2\n",
    "$$\n",
    "\n",
    "где $X$ — это матрица объекты-признаки, а $y$ — вектор правильных ответов\n",
    "\n",
    "Для того чтобы воспользоваться методом градиентного спуска, нам нужно посчитать градиент нашего функционала. Для MSE он будет выглядеть так:\n",
    "\n",
    "$$\n",
    "\\nabla_w Q(w, X, y) = \\frac{2}{l} X^T(Xw-y)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05tP14CdBp0l"
   },
   "source": [
    "Ниже приведён базовый класс `BaseLoss`, который мы будем использовать для реализации всех наших функционалов ошибки (= функций потерь = лоссов). Менять его не нужно. У него есть два абстрактных метода:\n",
    "1. Метод `calc_loss`, который будет принимать на вход объекты `x`, правильные ответы `y` и веса `w` и вычислять значения функционала ошибки\n",
    "2. Метод `calc_grad`, который будет принимать на вход объекты `x`, правильные ответы `y` и веса `w` и вычислять значения градиента (вектор)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JWmOOrj6Bp0m"
   },
   "outputs": [],
   "source": [
    "import abc\n",
    "\n",
    "\n",
    "class BaseLoss(abc.ABC):\n",
    "    \"\"\"Базовый класс лосса\"\"\"\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def calc_loss(self, X: np.ndarray, y: np.ndarray, w: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Функция для вычислений значения лосса\n",
    "        :param X: np.ndarray размера (n_objects, n_features) с объектами датасета\n",
    "        :param y: np.ndarray размера (n_objects,) с правильными ответами\n",
    "        :param w: np.ndarray размера (n_features,) с весами линейной регрессии\n",
    "        :return: число -- значения функции потерь\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def calc_grad(self, X: np.ndarray, y: np.ndarray, w: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Функция для вычислений градиента лосса по весам w\n",
    "        :param X: np.ndarray размера (n_objects, n_features) с объектами датасета\n",
    "        :param y: np.ndarray размера (n_objects,) с правильными ответами\n",
    "        :param w: np.ndarray размера (n_features,) с весами линейной регрессии\n",
    "        :return: np.ndarray размера (n_features,) градиент функции потерь по весам w\n",
    "        \"\"\"\n",
    "        raise NotImplementedError"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ob65L_82Bp0m"
   },
   "source": [
    "Реализация этого абстрактного класса: Mean Squared Error лосс."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezASiSCgBp0m"
   },
   "source": [
    "**Задание 1.1:** __Реализуйте класс `MSELoss`__\n",
    "\n",
    "Он должен вычислять значение функционала ошибки (лосс) $\n",
    "Q(w, X, y)$ и его градиент $\\nabla_w Q(w, X, y)$ по формулам (выше)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpvu5XryBp0m"
   },
   "outputs": [],
   "source": [
    "class MSELoss(BaseLoss):\n",
    "    def calc_loss(self, X: np.ndarray, y: np.ndarray, w: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Функция для вычислений значения лосса\n",
    "        :param X: np.ndarray размера (n_objects, n_features) с объектами датасета\n",
    "        :param y: np.ndarray размера (n_objects,) с правильными ответами\n",
    "        :param w: np.ndarray размера (n_features,) с весами линейной регрессии\n",
    "        :return: число -- значения функции потерь\n",
    "        \"\"\"\n",
    "        # -- YOUR CODE HERE --\n",
    "        # Вычислите значение функции потерь при помощи X, y и w и верните его   \n",
    "  \n",
    "\n",
    "    def calc_grad(self, X: np.ndarray, y: np.ndarray, w: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Функция для вычислений градиента лосса по весам w\n",
    "        :param X: np.ndarray размера (n_objects, n_features) с объектами датасета\n",
    "        :param y: np.ndarray размера (n_objects,) с правильными ответами\n",
    "        :param w: np.ndarray размера (n_features,) с весами линейной регрессии\n",
    "        :return: np.ndarray размера (n_features,) градиент функции потерь по весам w\n",
    "        \"\"\"\n",
    "        # -- YOUR CODE HERE --\n",
    "        # Вычислите значение вектора градиента при помощи X, y и w и верните его\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "188svk-jBp0n"
   },
   "source": [
    "Теперь мы можем создать объект `MSELoss` и при помощи него вычислять значение нашего функционала ошибки и градиент:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yoQZnHVRBp0n"
   },
   "outputs": [],
   "source": [
    "# Создадим объект лосса\n",
    "loss = MSELoss()\n",
    "\n",
    "# Создадим датасет\n",
    "X = np.arange(200).reshape(20, 10)\n",
    "y = np.arange(20)\n",
    "\n",
    "# Создадим вектор весов\n",
    "w = np.arange(10)\n",
    "\n",
    "#print(X)\n",
    "#print(y)\n",
    "#print(w)\n",
    "\n",
    "# Выведем значение лосса и градиента на этом датасете с этим вектором весов\n",
    "print(loss.calc_loss(X, y, w))\n",
    "print(loss.calc_grad(X, y, w))\n",
    "\n",
    "# Проверка, что методы реализованы правильно\n",
    "assert loss.calc_loss(X, y, w) == 27410283.5, \"Метод calc_loss реализован неверно\"\n",
    "assert np.allclose(\n",
    "    loss.calc_grad(X, y, w),\n",
    "    np.array(\n",
    "        [\n",
    "            1163180.0,\n",
    "            1172281.0,\n",
    "            1181382.0,\n",
    "            1190483.0,\n",
    "            1199584.0,\n",
    "            1208685.0,\n",
    "            1217786.0,\n",
    "            1226887.0,\n",
    "            1235988.0,\n",
    "            1245089.0,\n",
    "        ]\n",
    "    ),\n",
    "), \"Метод calc_grad реализован неверно\"\n",
    "print(\"Всё верно!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wqRkUQpABp0n"
   },
   "source": [
    "Для вычисления градиента все готово, реализуем градиентный спуск. Напомним, что формула для одной итерации градиентного спуска выглядит следующим \n",
    "образом:\n",
    "\n",
    "$$\n",
    "w^t = w^{t-1} - \\eta \\nabla_{w} Q(w^{t-1}, X, y)\n",
    "$$\n",
    "\n",
    "Где $w^t$ — значение вектора весов на $t$-ой итерации, а $\\eta$ — параметр learning rate, отвечающий за размер шага."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i9gOWGzqBp0n"
   },
   "source": [
    "**Задание 1.2:** __Реализуйте функцию `gradient_descent`__\n",
    "\n",
    "Функция должна принимать на вход начальное значение весов линейной модели `w_init`, матрицу объектов-признаков `X`, \n",
    "вектор правильных ответов `y`, объект функции потерь `loss`, размер шага `lr` и количество итераций `n_iterations`.\n",
    "\n",
    "Функция должна реализовывать цикл, в котором осуществляется шаг градиентного спуска (градиенты берутся из `loss` посредством вызова метода `calc_grad`) по формуле выше и возвращать \n",
    "траекторию спуска (список из новых значений весов на каждом шаге)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Wxu_8RHkBp0o"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgradient_descent\u001b[39m(\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m     w_init: \u001b[43mnp\u001b[49m.ndarray,\n\u001b[32m      3\u001b[39m     X: np.ndarray,\n\u001b[32m      4\u001b[39m     y: np.ndarray,\n\u001b[32m      5\u001b[39m     loss: BaseLoss,\n\u001b[32m      6\u001b[39m     lr: \u001b[38;5;28mfloat\u001b[39m,\n\u001b[32m      7\u001b[39m     n_iterations: \u001b[38;5;28mint\u001b[39m = \u001b[32m100000\u001b[39m,\n\u001b[32m      8\u001b[39m ) -> List[np.ndarray]:\n\u001b[32m      9\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[33;03m    Функция градиентного спуска\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[33;03m    :param w_init: np.ndarray размера (n_feratures,) -- начальное значение вектора весов\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     17\u001b[39m \u001b[33;03m    :return: Список из n_iterations объектов np.ndarray размера (n_features,) -- история весов на каждом шаге\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m     19\u001b[39m     \u001b[38;5;66;03m# -- YOUR CODE HERE --\u001b[39;00m\n",
      "\u001b[31mNameError\u001b[39m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "def gradient_descent(\n",
    "    w_init: np.ndarray,\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    loss: BaseLoss,\n",
    "    lr: float,\n",
    "    n_iterations: int = 100000,\n",
    ") -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Функция градиентного спуска\n",
    "    :param w_init: np.ndarray размера (n_feratures,) -- начальное значение вектора весов\n",
    "    :param X: np.ndarray размера (n_objects, n_features) -- матрица объекты-признаки\n",
    "    :param y: np.ndarray размера (n_objects,) -- вектор правильных ответов\n",
    "    :param loss: Объект подкласса BaseLoss, который умеет считать градиенты при помощи loss.calc_grad(X, y, w)\n",
    "    :param lr: float -- параметр величины шага, на который нужно домножать градиент\n",
    "    :param n_iterations: int -- сколько итераций делать\n",
    "    :return: Список из n_iterations объектов np.ndarray размера (n_features,) -- история весов на каждом шаге\n",
    "    \"\"\"\n",
    "    # -- YOUR CODE HERE --\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fM-gLeXPBp0o"
   },
   "source": [
    "Теперь создадим синтетический датасет и функцию, которая будет рисовать траекторию градиентного спуска по истории.  \n",
    "(Если не оговорено иное, то в задачах используются указанные параметры `n_features,\n",
    "n_objects, batch_size, num_steps`). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GgaCBMiDBp0o"
   },
   "outputs": [],
   "source": [
    "# Создаём датасет из двух переменных и реального вектора зависимости w_true\n",
    "\n",
    "np.random.seed(1337)\n",
    "\n",
    "n_features = 2\n",
    "n_objects = 300\n",
    "batch_size = 10\n",
    "num_steps = 43\n",
    "\n",
    "w_true = np.random.normal(size=(n_features,))\n",
    "\n",
    "X = np.random.uniform(-5, 5, (n_objects, n_features))\n",
    "X *= (np.arange(n_features) * 2 + 1)[np.newaxis, :]\n",
    "y = X.dot(w_true) + np.random.normal(0, 1, (n_objects))\n",
    "w_init = np.random.uniform(-2, 2, (n_features))\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YCsiY4PBBp0o"
   },
   "outputs": [],
   "source": [
    "loss = MSELoss()\n",
    "w_list = gradient_descent(w_init, X, y, loss, 0.01, num_steps)\n",
    "print(loss.calc_loss(X, y, w_list[0]))\n",
    "print(loss.calc_loss(X, y, w_list[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kZm3EqMwBp0p"
   },
   "outputs": [],
   "source": [
    "def plot_gd(w_list: Iterable, X: np.ndarray, y: np.ndarray, loss: BaseLoss):\n",
    "    \"\"\"\n",
    "    Функция для отрисовки траектории градиентного спуска\n",
    "    :param w_list: Список из объектов np.ndarray размера (n_features,) -- история весов на каждом шаге\n",
    "    :param X: np.ndarray размера (n_objects, n_features) -- матрица объекты-признаки\n",
    "    :param y: np.ndarray размера (n_objects,) -- вектор правильных ответов\n",
    "    :param loss: Объект подкласса BaseLoss, который умеет считать лосс при помощи loss.calc_loss(X, y, w)\n",
    "    \"\"\"\n",
    "    w_list = np.array(w_list)\n",
    "    meshgrid_space = np.linspace(-2, 2, 100)\n",
    "    A, B = np.meshgrid(meshgrid_space, meshgrid_space)\n",
    "\n",
    "    levels = np.empty_like(A)\n",
    "    for i in range(A.shape[0]):\n",
    "        for j in range(A.shape[1]):\n",
    "            w_tmp = np.array([A[i, j], B[i, j]])\n",
    "            levels[i, j] = loss.calc_loss(X, y, w_tmp)\n",
    "\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    plt.title(\"GD trajectory\")\n",
    "    plt.xlabel(r\"$w_1$\")\n",
    "    plt.ylabel(r\"$w_2$\")\n",
    "    plt.xlim(w_list[:, 0].min() - 0.1, w_list[:, 0].max() + 0.1)\n",
    "    plt.ylim(w_list[:, 1].min() - 0.1, w_list[:, 1].max() + 0.1)\n",
    "    plt.gca().set_aspect(\"equal\")\n",
    "\n",
    "    # visualize the level set\n",
    "    CS = plt.contour(\n",
    "        A, B, levels, levels=np.logspace(0, 1, num=20), cmap=plt.cm.rainbow_r\n",
    "    )\n",
    "    CB = plt.colorbar(CS, shrink=0.8, extend=\"both\")\n",
    "\n",
    "    # visualize trajectory\n",
    "    plt.scatter(w_list[:, 0], w_list[:, 1])\n",
    "    plt.plot(w_list[:, 0], w_list[:, 1])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JqufHS3PBp0p"
   },
   "source": [
    "**Задание 1.3:** __При помощи функций `gradient_descent` и  `plot_gd` нарисуйте траекторию градиентного спуска для разных значений длины шага (параметра `lr`)__. Используйте не менее четырёх разных значений для `lr`. Для каждой длины шага вычисляйтете значение функционала ошибки при помощи метода `calc_loss` на первой и последней итерациях градиентного спуска.\n",
    "\n",
    "Сделайте и опишите свои выводы о том, как параметр `lr` влияет на поведение градиентного спуска\n",
    "\n",
    "Подсказки:\n",
    "* Функция `gradient_descent` возвращает историю весов, которую нужно подать в функцию `plot_gd`\n",
    "* Хорошие значения для `lr` могут лежать в промежутке от 0.0001 до 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hPtpweFLBp0q"
   },
   "outputs": [],
   "source": [
    "# -- YOUR CODE HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JdxxpTlSBp0q"
   },
   "source": [
    "Теперь реализуем стохастический градиентный спуск"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bNXcI8AqBp0q"
   },
   "source": [
    "**Задание 1.4:** __Реализуйте функцию `stochastic_gradient_descent`__\n",
    "\n",
    "Функция должна принимать все те же параметры, что и функция `gradient_descent`, но ещё параметр `batch_size`, отвечающий за размер батча. \n",
    "\n",
    "Функция должна как и раньше реализовывать цикл, в котором происходит шаг градиентного спуска, но на каждом шаге считать градиент не по всей выборке `X`, а только по случайно выбранной части.\n",
    "\n",
    "Подсказка: для выбора случайной части можно использовать [`np.random.choice`](https://numpy.org/doc/stable/reference/random/generated/numpy.random.choice.html) с правильным параметром `size`, чтобы выбрать случайные индексы, а потом проиндексировать получившимся массивом массив `X`:\n",
    "```\n",
    "batch_indices = np.random.choice(X.shape[0], size=batch_size, replace=False)\n",
    "batch = X[batch_indices]\n",
    "```\n",
    "\n",
    "(*здесь np.random.choice генерирует из np.arange(X.shape[0]), длина size,\n",
    "replace=False - без повторения*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RNlOVmKLBp0q"
   },
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(\n",
    "    w_init: np.ndarray,\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    loss: BaseLoss,\n",
    "    lr: float,\n",
    "    batch_size: int,\n",
    "    n_iterations: int = 1000,\n",
    ") -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Функция градиентного спуска\n",
    "    :param w_init: np.ndarray размера (n_feratures,) -- начальное значение вектора весов\n",
    "    :param X: np.ndarray размера (n_objects, n_features) -- матрица объекты-признаки\n",
    "    :param y: np.ndarray размера (n_objects,) -- вектор правильных ответов\n",
    "    :param loss: Объект подкласса BaseLoss, который умеет считать градиенты при помощи loss.calc_grad(X, y, w)\n",
    "    :param lr: float -- параметр величины шага, на который нужно домножать градиент\n",
    "    :param batch_size: int -- размер подвыборки, которую нужно семплировать на каждом шаге\n",
    "    :param n_iterations: int -- сколько итераций делать\n",
    "    :return: Список из n_iterations объектов np.ndarray размера (n_features,) -- история весов на каждом шаге\n",
    "    \"\"\"\n",
    "    # -- YOUR CODE HERE --\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OhnQEuAFBp0r"
   },
   "source": [
    "**Задание 1.5:** __При помощи функций `stochastic_gradient_descent` и  `plot_gd` нарисуйте траекторию градиентного спуска для разных значений длины шага (параметра `lr`) и размера подвыборки (параметра `batch_size`)__. Используйте не менее трех разных значений для `lr` и `batch_size`. (Для каждых длины шага и размера подвыборки вычисляйтете значение функционала ошибки при помощи метода `calc_loss` на первой и последней итерациях стохастического градиентного спуска).\n",
    "\n",
    "Сделайте и опишите свои выводы о том, как параметры  `lr` и `batch_size` влияют на поведение стохастического градиентного спуска. Как отличается поведение стохастического градиентного спуска от обычного?\n",
    "\n",
    "Обратите внимание, что в нашем датасете всего 300 объектов, так что `batch_size` больше этого числа не будет иметь смысла."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ysPrEpkjBp0r"
   },
   "outputs": [],
   "source": [
    "# -- YOUR CODE HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ZWZE2VDBp0r"
   },
   "source": [
    "Можно заметить, что поведение градиентного спуска, особенно его стохастической версии, очень сильно зависит от размера шага. \n",
    "\n",
    "Как правило, в начале спуска мы хотим делать большие шаги, чтобы поскорее подойти поближе к минимуму, а позже мы уже хотим делать шаги маленькие, чтобы более точнее этого минимума достичь и не \"перепрыгнуть\" его. \n",
    "\n",
    "Чтобы достичь такого поведения мы можем постепенно уменьшать длину шага с увеличением номера итерации. Сделать это можно, например, вычисляя на каждой итерации длину шага по следующей формуле:\n",
    "\n",
    "$$\n",
    "    \\eta_t\n",
    "    =\n",
    "    \\lambda\n",
    "    \\left(\n",
    "        \\frac{s_0}{s_0 + t}\n",
    "    \\right)^p\n",
    "$$\n",
    "\n",
    "где $\\eta_t$ — длина шага на итерации $t$, $\\lambda$ — начальная длина шага (параметр `lr` у нас), $s_0$ и $p$ — настраиваемые параметры."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9Hqj3zJ4Bp0r"
   },
   "source": [
    "**Задание 1.6:** __Реализуйте функцию `stochastic_gradient_descent` с затухающим шагом по формуле выше__. Параметр $s_0$ возьмите равным 1. Параметр $p$ возьмите из нового аргумента функции `p`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7uL65tOmBp0r"
   },
   "outputs": [],
   "source": [
    "def stochastic_gradient_descent(\n",
    "    w_init: np.ndarray,\n",
    "    X: np.ndarray,\n",
    "    y: np.ndarray,\n",
    "    loss: BaseLoss,\n",
    "    lr: float,\n",
    "    batch_size: int,\n",
    "    p: float,\n",
    "    n_iterations: int = 1000,\n",
    ") -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Функция градиентного спуска\n",
    "    :param w_init: np.ndarray размера (n_feratures,) -- начальное значение вектора весов\n",
    "    :param X: np.ndarray размера (n_objects, n_features) -- матрица объекты-признаки\n",
    "    :param y: np.ndarray размера (n_objects,) -- вектор правильных ответов\n",
    "    :param loss: Объект подкласса BaseLoss, который умеет считать градиенты при помощи loss.calc_grad(X, y, w)\n",
    "    :param lr: float -- параметр величины шага, на который нужно домножать градиент\n",
    "    :param batch_size: int -- размер подвыборки, которую нужно семплировать на каждом шаге\n",
    "    :param p: float -- значение степени в формуле затухания длины шага\n",
    "    :param n_iterations: int -- сколько итераций делать\n",
    "    :return: Список из n_iterations объектов np.ndarray размера (n_features,) -- история весов на каждом шаге\n",
    "    \"\"\"\n",
    "    # -- YOUR CODE HERE --\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IwgaqqB5Bp0r"
   },
   "source": [
    "**Задание 1.7:** __При помощи новой функции `stochastic_gradient_descent` и функции `plot_gd` нарисуйте траекторию градиентного спуска для разных значений параметра `p`__. Используйте не менее четырёх разных значений для `p`. Хорошими могут быть значения, лежащие в промежутке от 0.1 до 1.\n",
    "Параметр `lr` возьмите равным 0.01, а параметр `batch_size` равным 10. (Для каждого значения параметра `p` вычисляйте значение функционала ошибки при помощи метода `calc_loss` на первой и последней итерациях стохастического градиентного спуска).\n",
    "\n",
    "Сделайте и опишите свои выводы о том, как параметр `p` влияет на поведение стохастического градиентного спуска"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ri2bF6iCBp0s"
   },
   "outputs": [],
   "source": [
    "# -- YOUR CODE HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x0YKLC4mBp0s"
   },
   "source": [
    "**Задание 1.8:** __Сравните сходимость обычного градиентного спуска и его стохастической версии__:\n",
    "Нарисуйте график зависимости значения функционала ошибки (лосса) (его можно посчитать при помощи метода `calc_loss`, используя $x$ и $y$ из датасета и $w$ с соответствующей итерации) от номера итерации для траекторий, полученных при помощи обычного и стохастического градиентного спуска __с одинаковыми параметрами__. В SGD параметр `batch_size` возьмите равным 10, `p=0`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(n_features)\n",
    "print(n_objects)\n",
    "print(batch_size)\n",
    "print(num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O_f0qCQsBp0s"
   },
   "outputs": [],
   "source": [
    "# -- YOUR CODE HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Часть 2. Линейная регрессия "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, List"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создадим класс для линейной регрессии. Он будет использовать интерфейс, знакомый нам из библиотеки `sklearn`.\n",
    "\n",
    "В методе `fit` мы будем подбирать веса `w` при помощи градиентного спуска нашим методом `gradient_descent`.\n",
    "\n",
    "В методе `predict` мы будем применять нашу регрессию к датасету, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 2.1:** Допишите код в методах `fit` и `predict` класса `LinearRegression_1`\n",
    "\n",
    "В методе `fit` вам нужно инициализировать веса `w` (например, из члучайного распределения), применить наш `gradient_descent` и сохранить последние веса `w` из траектории.\n",
    "\n",
    "В методе `predict` вам нужно применить линейную регрессию и вернуть вектор ответов.\n",
    "\n",
    "Обратите внимание, что объект лосса (функционала ошибки) передаётся в момент инициализации и хранится в `self.loss`. Его нужно использовать в `fit` для `gradient_descent` (например, с `n_iterations: int = 1000`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegression_1:\n",
    "    def __init__(self, loss: BaseLoss, lr: float = 0.01) -> None: \n",
    "        #loss - функционал ошибки\n",
    "        #lr - градиентный шаг\n",
    "        self.loss = loss\n",
    "        self.lr = lr\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray) -> \"LinearRegression_1\":\n",
    "        X = np.asarray(X)\n",
    "        y = np.asarray(y)\n",
    "        # Добавляем столбец из единиц для константного признака\n",
    "        X = np.hstack([X, np.ones([X.shape[0], 1])])\n",
    "\n",
    "        # -- YOUR CODE HERE --  \n",
    "        \n",
    "        return self      \n",
    "\n",
    "    \n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        # Проверяем, что регрессия обучена, то есть, что был вызван fit и в нём был установлен атрибут self.w\n",
    "        assert hasattr(self, \"w\"), \"Linear regression must be fitted first\"\n",
    "        # Добавляем столбец из единиц для константного признака\n",
    "        X = np.hstack([X, np.ones([X.shape[0], 1])])\n",
    "\n",
    "        # -- YOUR CODE HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Класс линейной регрессии создан. Более того, мы можем управлять тем, какой функционал ошибки мы оптимизируем, просто передавая разные классы в параметр `loss` при инициализации. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Будем применять нашу регрессию к реальному датасету. Загрузим датасет с машинами (см. семинар 5_sem-sklearn-knn.ipynb):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "X_raw = pd.read_csv(\n",
    "    \"http://archive.ics.uci.edu/ml/machine-learning-databases/autos/imports-85.data\",\n",
    "    header=None,      #в исх. таблице нет названий колонок\n",
    "    na_values=[\"?\"],  #если ?, то NaN\n",
    ")\n",
    "\n",
    "X_raw.head()\n",
    "X_raw = X_raw[~X_raw[25].isna()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = X_raw[25]\n",
    "X_raw = X_raw.drop(25, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 2.2:** Обработайте датасет нужными методами, чтобы на нём можно было обучать линейную регрессию (см. семинар 5_sem-sklearn-knn.ipynb):\n",
    "\n",
    "* Заполните пропуски средними (библиотека SimpleImputer)\n",
    "* Переведите категориальные признаки в числовые (в методе get_dummies использовать drop_first=True.)\n",
    "* Разделите датасет на обучающую и тестовую выборку (задать: доля тестовой выборки равна 0.3, `random_state=42`, `shuffle=True`)\n",
    "* Нормализуйте числовые признаки (при помощи бибилиотеки StandardScaler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- YOUR CODE HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 2.3:** Обучите написанную вами линейную регрессию на обучающей выборке"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Создаем объект линейной регрессии для `MSELoss`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_1 = LinearRegression_1(MSELoss()) #создаем регрессор"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- YOUR CODE HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 2.4:** Посчитайте ошибку обученной регрессии на обучающей и тестовой выборке при помощи методов `mean_squared_error` и `r2_score` из `sklearn.metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- YOUR CODE HERE --"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Добавим к модели L2 регуляризацию (для борьбы с переобучением). Для этого нам нужно создать новый класс для нового функционала ошибки и его градиента.\n",
    "\n",
    "Формула функционала ошибки для MSE с L2 регуляризацией выглядит так:\n",
    "$$\n",
    "Q(w, X, y) = \\frac{1}{\\ell} \\sum\\limits_{i=1}^\\ell (\\langle x_i, w \\rangle - y_i)^2 + \\lambda ||w||^2\n",
    "$$\n",
    "\n",
    "Или в матричном виде:\n",
    "\n",
    "$$\n",
    "Q(w, X, y) = \\frac{1}{\\ell} || Xw - y ||^2 + \\lambda ||w||^2,\n",
    "$$\n",
    "\n",
    "где $\\lambda$ — коэффициент регуляризации\n",
    "\n",
    "Заметим, что (удобно для вычислений):\n",
    "$$\n",
    "Q(w, X, y) = \\frac{1}{\\ell} || Xw - y ||^2+\\lambda ||w||^2 =\\frac{1}{l} (y - Xw)^T(y-Xw)+ \\lambda w^Tw.\n",
    "$$\n",
    "\n",
    "Градиент $\\nabla_w Q(w, X, y)$ выглядит так:\n",
    "\n",
    "$$\n",
    "\\nabla_w Q(w, X, y) = \\frac{2}{\\ell} X^T(Xw-y) + 2 \\lambda w\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 2.5:** Реализуйте класс `MSEL2Loss`\n",
    "\n",
    "Он должен вычислять значение функционала ошибки (лосс) $\n",
    "Q(w, X, y)$ и его градиент $\\nabla_w Q(w, X, y)$ по формулам (выше).\n",
    "\n",
    "Подсказка: обратите внимание, что последний элемент вектора `w` — это bais (сдвиг) (в классе `LinearRegression` к матрице `X` добавляется колонка из единиц — константный признак). bais регуляризовать не нужно. Поэтому не забудьте убрать последний элемент из `w` при подсчёте слагаемого $\\lambda||w||^2$ в `calc_loss` и занулить его при подсчёте слагаемого $2 \\lambda w$ в `calc_grad`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSEL2Loss(BaseLoss):\n",
    "    def __init__(self, coef: float = 1.0):\n",
    "        \"\"\"\n",
    "        :param coef: коэффициент регуляризации (лямбда в формуле)\n",
    "        \"\"\"\n",
    "        self.coef = coef\n",
    "\n",
    "    def calc_loss(self, X: np.ndarray, y: np.ndarray, w: np.ndarray) -> float:\n",
    "        \"\"\"\n",
    "        Функция для вычислений значения лосса\n",
    "        :param X: np.ndarray размера (n_objects, n_features) с объектами датасета. Последний признак константный.\n",
    "        :param y: np.ndarray размера (n_objects,) с правильными ответами\n",
    "        :param w: np.ndarray размера (n_features,) с весами линейной регрессии. Последний вес -- bias.\n",
    "        :output: число -- значения функции потерь\n",
    "        \"\"\"    \n",
    "        # -- YOUR CODE HERE --\n",
    "        # Вычислите значение функции потерь при помощи X, y и w и верните его\n",
    "\n",
    "    def calc_grad(self, X: np.ndarray, y: np.ndarray, w: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Функция для вычислений градиента лосса по весам w\n",
    "        :param X: np.ndarray размера (n_objects, n_features) с объектами датасета\n",
    "        :param y: np.ndarray размера (n_objects,) с правильными ответами\n",
    "        :param w: np.ndarray размера (n_features,) с весами линейной регрессии\n",
    "        :output: np.ndarray размера (n_features,) градиент функции потерь по весам w\n",
    "        \"\"\"\n",
    "        # -- YOUR CODE HERE --\n",
    "        # Вычислите значение вектора градиента при помощи X, y и w и верните его\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Проверка:\n",
    "#Создадим объект лосса\n",
    "losst = MSEL2Loss()\n",
    "\n",
    "# Создадим датасет\n",
    "Xt = np.arange(200).reshape(20, 10)\n",
    "yt = np.arange(20)\n",
    "\n",
    "# Создадим вектор весов\n",
    "wt = np.arange(10)\n",
    "\n",
    "#print(Xt)\n",
    "#print(yt)\n",
    "#print(wt)\n",
    "\n",
    "# Выведем значение лосса и градиента на этом датасете с этим вектором весов\n",
    "print(losst.calc_loss(Xt, yt, wt))\n",
    "print(losst.calc_grad(Xt, yt, wt))\n",
    "\n",
    "# Проверка, что методы реализованы правильно\n",
    "assert losst.calc_loss(Xt, yt, wt) == 27410487.5, \"Метод calc_loss реализован неверно\" \n",
    "assert np.allclose(\n",
    "    losst.calc_grad(Xt, yt, wt),\n",
    "    np.array(\n",
    "        [\n",
    "            1163180.0,\n",
    "            1172283.0,\n",
    "            1181386.0,\n",
    "            1190489.0,\n",
    "            1199592.0,\n",
    "            1208695.0,\n",
    "            1217798.0,\n",
    "            1226901.0,\n",
    "            1236004.0,\n",
    "            1245089.0,\n",
    "        ]\n",
    "    ),\n",
    "), \"Метод calc_grad реализован неверно\"\n",
    "\n",
    "print(\"Всё верно!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы можем использовать лосс с l2 регуляризацией в нашей регрессии. Пусть:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_1_l2 = LinearRegression_1(MSEL2Loss(0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задание 2.6:** Обучите регрессию с лоссом `MSEL2Loss`. Попробуйте использовать другие коэффициенты регуляризации. Получилось ли улучшить разультат на тестовой выборке? Сравните результат применения регрессии с регуляризацией к обучающей и тестовой выборкам с результатом применения регрессии без регуляризации к тем же выборкам.(Для оценки качества использовать методы `mean_squared_error` и `r2_score` из `sklearn.metrics`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -- YOUR CODE HERE --"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "l5cGx9vyBp0x"
   ],
   "name": "hw05-gd.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
